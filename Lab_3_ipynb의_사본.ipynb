{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YisakH/2022_ML/blob/main/Lab_3_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC91mF_u6o-s"
      },
      "source": [
        "# Machine Learning - Lab Session 3\n",
        "\n",
        "This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. \n",
        "\n",
        "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
        "\n",
        "### Submitting your work:\n",
        "- <font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.\n",
        "- Commit the `.ipynb` file on your github repository and submit URL on E-Ruri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5bL80Gdm-7TI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C62c6fKX98aM"
      },
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnm87-pp6o-v"
      },
      "source": [
        "### Load MNIST Data\n",
        "We temperaly use `sklearng.datasets` for load MNIST before we learning `pytorch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cellView": "both",
        "id": "JLpLa8Jt7Vu4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "X = X / 255.0\n",
        "y = y.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pv_1sPkqByzq",
        "outputId": "50a822ce-feb6-429b-a6ef-c9079043e6c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215686\n",
            " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
            " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313725\n",
            " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.1372549  0.94509804\n",
            " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            " 0.58823529 0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.58039216\n",
            " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058824\n",
            " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
            " 0.31372549 0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333333 0.99215686\n",
            " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "plt.imshow(X[0].reshape((28, 28)), cmap='gray')\n",
        "plt.show()\n",
        "print(X[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7aHrm6nGDMB"
      },
      "source": [
        "### Preprocessing\n",
        "Reformat into a shape that's more adapted to the models we're going to train:\n",
        "1. labels as float 1-hot encodings. [hint, use [`np.eye`](https://numpy.org/devdocs/reference/generated/numpy.eye.html)] **[pts. 5]**\n",
        "2. split the data into train and test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7TnyPMF_CuM",
        "outputId": "db3e6e5c-b79c-499d-eb2a-0af36f4fa646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before encoding: 5\n",
            "after encoding: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# one-hot encoding\n",
        "print('before encoding: %s'% y[0])\n",
        "y = np.eye(10)[y]\n",
        "print('after encoding: %s' % y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRSyYiIIGIzS",
        "outputId": "5a125e78-f5c0-43b0-e0f9-7a33cd8f9fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set (56000, 784) (56000, 10)\n",
            "Test set (14000, 784) (14000, 10)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train partition and test partition\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=0, test_size=0.2)\n",
        "\n",
        "IMAGE_SIZE = 28\n",
        "NUM_LABELS = 10\n",
        "\n",
        "print('Training set', X_train.shape, y_train.shape)\n",
        "print('Test set', X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lw7LP8x_OWi"
      },
      "source": [
        "In this lab session, we only use partial data for training and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jNaDkIi46o-y"
      },
      "outputs": [],
      "source": [
        "DATA_SIZE = 100\n",
        "\n",
        "X_train = X_train[0:DATA_SIZE]\n",
        "y_train = y_train[0:DATA_SIZE]\n",
        "\n",
        "X_test = X_test[0:DATA_SIZE]\n",
        "y_test = y_test[0:DATA_SIZE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu-bNfMf6o-z"
      },
      "source": [
        "## Training a Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewBirlzP6o-1"
      },
      "source": [
        "### Activation Function\n",
        "\n",
        "First let's implement two activation function, `sigmoid` and `softmax`. Also the derivation of `sigmoid` function. **[pts. 5 per function]**\n",
        "$$\n",
        "\\begin{align}\n",
        "\\sigma(x) &=  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} } \\\\\n",
        "\\mathrm{softmax}(x_{i}) &= \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\\\\n",
        "\\sigma'(x) &= \\sigma(x)\\sigma(1-x) \\\\\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "m_9pl55P6o-1"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "\n",
        "def softmax(Z):\n",
        "    return np.exp(Z)/(np.exp(Z).sum(axis=0, keepdims=True))\n",
        "\n",
        "def sigmoid_derivative(Z):\n",
        "    return sigmoid(Z)*(1-sigmoid(Z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpeOlzYaAjVU",
        "outputId": "b9590a5b-672c-4ffd-b091-e4d22dd4117d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.52497918747894\n",
            "[0.26175419 0.50140083 0.23684498]\n",
            "0.24937604019289197\n"
          ]
        }
      ],
      "source": [
        "# test functions\n",
        "print(sigmoid(0.1))\n",
        "print(softmax([0.15, 0.8, 0.05]))\n",
        "print(sigmoid_derivative(0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH3YKCDC6o-z"
      },
      "source": [
        "Let's now build a neural network with one input layer, one hidden layer, and one output layer. The number of nodes in the input layer is determined by the dimensionality of our data, 784. Similarly, the number of nodes in the output layer is determined by the number of classes we have, 10. The input to the network will be the pixel values of the input image and its output will be ten probabilities, ones for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B1bQjCm_xkL"
      },
      "source": [
        "![Sample network](https://drive.google.com/uc?id=1D1XuhvNokK5S5yn8V34JGvyE-XXz9rST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObPwuWqv6o-z"
      },
      "source": [
        "### How our network makes predictions\n",
        "\n",
        "Our network makes predictions using *forward propagation*, which is just a bunch of matrix multiplications and the application of the activation function(s) we defined above. If $x$ is the 784-dimensional input to our network then we calculate our prediction $\\hat{y}$ (ten-dimensional) as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJBKF3zf6o-z"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{s}_1 & = W_1\\mathbf{x} + \\mathbf{b}_1 \\\\\n",
        "\\mathbf{z}_1 & = \\mathrm{sigmoid}(\\mathbf{s}_1) \\\\\n",
        "\\mathbf{s}_2 & = W_2\\mathbf{z}_1 + \\mathbf{b}_2 \\\\\n",
        "\\mathbf{h} & = \\hat{y} = \\mathrm{softmax}(\\mathbf{s}_2)\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3J5N6A6o-0"
      },
      "source": [
        "$z_i$ is the input of layer $i$ and $a_i$ is the output of layer $i$ after applying the activation function. $W_1, b_1, W_2, b_2$ are  parameters of our network, which we need to learn from our training data. You can think of them as matrices transforming data between layers of the network. Looking at the matrix multiplications above we can figure out the dimensionality of these matrices.\n",
        "\n",
        "If we use 1024 nodes for our hidden layer then $W_1 \\in \\mathbb{R}^{1024\\times784}$, $b_1 \\in \\mathbb{R}^{1024\\times1}$, $W_2 \\in \\mathbb{R}^{10\\times1024}$, $b_2 \\in \\mathbb{R}^{10\\times1}$. Now you see why we have more parameters if we increase the size of the hidden layer.\n",
        "\n",
        "**[note]** beware of dimension of $\\mathbf{x}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pVFqhiNb_prh"
      },
      "outputs": [],
      "source": [
        "layers_size = [784, 50, 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDjwqr-uIXM6"
      },
      "source": [
        "Define parameters and initialize them with `np.random.rand` for W and `np.zeros` for b. **[pts. 5]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayJva00z_oB2",
        "outputId": "cabcf703-8427-455f-8be5-da97fe5bde3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 784)\n",
            "(50, 1)\n",
            "(10, 50)\n",
            "(10, 1)\n"
          ]
        }
      ],
      "source": [
        "# initialization\n",
        "np.random.seed(1)\n",
        "\n",
        "parameters = {}\n",
        "parameters[\"W1\"] = np.random.randn(layers_size[1], layers_size[0]) / np.sqrt(layers_size[0])\n",
        "parameters[\"b1\"] = np.zeros((layers_size[1], 1))\n",
        "\n",
        "parameters[\"W2\"] = np.random.randn(layers_size[2], layers_size[1]) / np.sqrt(layers_size[1])\n",
        "parameters[\"b2\"] = np.zeros((layers_size[2], 1))\n",
        "\n",
        "print(parameters[\"W1\"].shape)\n",
        "print(parameters[\"b1\"].shape)\n",
        "print(parameters[\"W2\"].shape)\n",
        "print(parameters[\"b2\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed forward $\\mathbf{x}$ through Networks. **[pts. 10]**"
      ],
      "metadata": {
        "id": "lorKSUwDYoNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vD-7B0fknV2",
        "outputId": "ccc53430-aaff-4739-fbab-96919457f4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 100)\n",
            "(10, 100)\n"
          ]
        }
      ],
      "source": [
        "# forward\n",
        "store = {}\n",
        "store['X'] = X_train.T\n",
        "\n",
        "## input to hidden\n",
        "S = parameters['W1'].dot(store['X']) + parameters['b1']\n",
        "Z = sigmoid(S)\n",
        "store[\"Z\"] = Z\n",
        "store[\"W1\"] = parameters[\"W1\"]\n",
        "store[\"S1\"] = S\n",
        "\n",
        "## hidden to output\n",
        "S = parameters['W2'].dot(Z) + parameters['b2']\n",
        "H = sigmoid(S)\n",
        "store[\"H\"] = H        \n",
        "store[\"W2\"] = parameters[\"W2\"]\n",
        "store[\"S2\"] = S      \n",
        "\n",
        "print(store[\"Z\"].shape)\n",
        "print(store[\"H\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtkfchpT6o-0"
      },
      "source": [
        "### Learning the Parameters\n",
        "\n",
        "Learning the parameters for our network means finding parameters ($W_1, b_1, W_2, b_2$) that minimize the error on our training data. But how do we define the error? We call the function that measures our error the *loss function*. A common choice with the softmax output is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression). If we have $N$ training examples and $C$ classes then the loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIiOvIyZ6o-0"
      },
      "source": [
        "The formula looks complicated, but all it really does is sum over our training examples and add to the loss if we predicted the incorrect class. So, the further away $y$ (the correct labels) and $\\hat{y}$ (our predictions) are, the greater our loss will be. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldCES_Px6o-0"
      },
      "source": [
        "Remember that our goal is to find the parameters that minimize our loss function. We can use [gradient descent](http://cs231n.github.io/optimization-1/) to find its minimum. I will implement the most vanilla version of gradient descent, also called batch gradient descent with a fixed learning rate. Variations such as SGD (stochastic gradient descent) or minibatch gradient descent typically perform better in practice. So if you are serious you'll want to use one of these, and ideally you would also [decay the learning rate over time](http://cs231n.github.io/neural-networks-3/#anneal).\n",
        "\n",
        "As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. To calculate these gradients we use the famous *backpropagation algorithm*, which is a way to efficiently calculate the gradients starting from the output. I won't go into detail how backpropagation works, but there are many excellent explanations ([here](http://colah.github.io/posts/2015-08-Backprop/) or [here](http://cs231n.github.io/optimization-2/)) floating around the web."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMCgWwWb6o-0"
      },
      "source": [
        "Applying the backpropagation formula we find the following (trust me on this): **[pts. 20]**\n",
        "\n",
        "backpropagation for output layer\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\mathbf{e} = \\hat{y} - y \\\\\n",
        "& \\delta_2 = \\mathbf{e} \\sigma'(\\mathbf{s}_2) \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{W_2}} = \\mathbf{z}^T \\delta_2  \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{b_2}} = \\delta_2 \\\\ \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "backpropagation for hidden layer\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\delta_1 = W_2^T\\delta_2 \\circ \\sigma'(\\mathbf{s}_1)  \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{W_1}} = \\mathbf{x}^T \\delta_1\\\\\n",
        "& \\frac{\\partial{L}}{\\partial{b_1}} = \\delta_1 \\\\\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJnCUVgepcx0",
        "outputId": "9bc02196-4994-4ebd-ff7a-da71580f8ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error (10, 100)\n",
            "store (10, 100)\n",
            "(50, 784)\n",
            "(50, 1)\n",
            "(10, 50)\n",
            "(10, 1)\n"
          ]
        }
      ],
      "source": [
        "n = X_train.shape[0]\n",
        "derivatives = {}\n",
        " \n",
        "# backward\n",
        "## output to hidden\n",
        "error = store[\"H\"] - y_train.T \n",
        "print('error', error.shape)\n",
        "print('store', store['S2'].shape)\n",
        "deleta2 = error * sigmoid_derivative(store[\"S2\"])\n",
        "\n",
        "dW = deleta2.dot(store['Z'].T)\n",
        "db = np.sum(deleta2, axis=1, keepdims=True)\n",
        "\n",
        "derivatives[\"dW2\"] = dW\n",
        "derivatives[\"db2\"] = db\n",
        "\n",
        "## hidden to output\n",
        "dAPrev = store[\"W2\"].T.dot(deleta2)\n",
        "deleta1 = dAPrev * sigmoid_derivative(store[\"S1\"])\n",
        "\n",
        "dW = deleta1.dot(store['X'].T)\n",
        "db = np.sum(deleta1, axis =1, keepdims=True)\n",
        "\n",
        "derivatives[\"dW1\"] = dW\n",
        "derivatives[\"db1\"] = db\n",
        "\n",
        "print(derivatives['dW1'].shape)\n",
        "print(derivatives['db1'].shape)\n",
        "print(derivatives['dW2'].shape)\n",
        "print(derivatives['db2'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jw22p14wap8"
      },
      "source": [
        "### Gradient Descent\n",
        "Update via gradient descent as follows: **[pts. 10]**\n",
        "$$\n",
        "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta ∇\\frac{\\partial \\varepsilon}{\\partial\\mathbf{w}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hkt5YtKAu_Zd"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "parameters[\"W2\"] = parameters['W2'] - learning_rate * derivatives['dW2']\n",
        "parameters[\"b2\"] = parameters['b2'] - learning_rate * derivatives['db2']\n",
        "\n",
        "parameters[\"W1\"] = parameters['W1'] - learning_rate * derivatives['dW1']\n",
        "parameters[\"b1\"] = parameters['b1'] - learning_rate * derivatives['db1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjXVLPGE6o-0"
      },
      "source": [
        "## Implementation of Whole Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUWHI8x6o-1"
      },
      "source": [
        "Finally, here comes the function to train our Artificial Neural Network as class. Complete the code. **[pts. 30]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-nIqBuT66o-1"
      },
      "outputs": [],
      "source": [
        "class ANN:\n",
        "    def __init__(self, layers_size):\n",
        "        self.layers_size = layers_size\n",
        "        self.parameters = {}\n",
        "        self.L = len(self.layers_size)\n",
        "        self.n = 0\n",
        "        self.costs = []\n",
        "        self.val_costs = []\n",
        " \n",
        "    def sigmoid(self, Z):\n",
        "        return sigmoid(Z)\n",
        " \n",
        "    def softmax(self, Z):\n",
        "        return softmax(Z)\n",
        "\n",
        "    def sigmoid_derivative(self, Z):\n",
        "        return sigmoid_derivative(Z)\n",
        " \n",
        "    def initialize_parameters(self):\n",
        "        np.random.seed(1)\n",
        "        ########### complete the code ################\n",
        "\n",
        "        self.parameters[\"W1\"] = np.random.randn(self.layers_size[1], self.layers_size[0]) / np.sqrt(self.layers_size[0])\n",
        "        self.parameters[\"b1\"] = np.zeros((self.layers_size[1], 1))\n",
        "\n",
        "        self.parameters[\"W2\"] = np.random.randn(self.layers_size[2], self.layers_size[1]) / np.sqrt(self.layers_size[1])\n",
        "        self.parameters[\"b2\"] = np.zeros((self.layers_size[2], 1))\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "    def forward(self, X):\n",
        "        store = {}\n",
        "        store['X'] = X.T\n",
        "        \n",
        "        ########### complete the code ################\n",
        "\n",
        "        S = parameters['W1'].dot(store['X']) + parameters['b1']\n",
        "        Z = sigmoid(S)\n",
        "        store[\"Z\"] = Z\n",
        "        store[\"W1\"] = parameters[\"W1\"]\n",
        "        store[\"S1\"] = S\n",
        "\n",
        "        ## hidden to output\n",
        "        S = parameters['W2'].dot(Z) + parameters['b2']\n",
        "        H = sigmoid(S)\n",
        "        store[\"H\"] = H        \n",
        "        store[\"W2\"] = parameters[\"W2\"]\n",
        "        store[\"S2\"] = S      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        return H, store\n",
        " \n",
        "    def backward(self, X, Y, store):\n",
        "        ########### complete the code ################\n",
        "\n",
        "        error = store[\"H\"] - y_train.T \n",
        "        deleta2 = error * sigmoid_derivative(store[\"S2\"])\n",
        "\n",
        "        dW = deleta2.dot(store['Z'].T)\n",
        "        db = np.sum(deleta2, axis=1, keepdims=True)\n",
        "\n",
        "        derivatives[\"dW2\"] = dW\n",
        "        derivatives[\"db2\"] = db\n",
        "\n",
        "        ## hidden to output\n",
        "        dAPrev = store[\"W2\"].T.dot(deleta2)\n",
        "        deleta1 = dAPrev * sigmoid_derivative(store[\"S1\"])\n",
        "\n",
        "        dW = deleta1.dot(store['X'].T)\n",
        "        db = np.sum(deleta1, axis =1, keepdims=True)\n",
        "\n",
        "        derivatives[\"dW1\"] = dW\n",
        "        derivatives[\"db1\"] = db\n",
        "\n",
        "        ##############################################\n",
        " \n",
        "        return derivatives\n",
        " \n",
        "    def fit(self, X, Y, learning_rate=0.01, n_iterations=2500):\n",
        "        np.random.seed(1)\n",
        " \n",
        "        self.n = X.shape[0]\n",
        "        self.layers_size.insert(0, X.shape[1])\n",
        "        self.initialize_parameters()\n",
        "\n",
        "        for loop in range(n_iterations):\n",
        "            H, store = self.forward(X)\n",
        "            cost = -np.mean(Y * np.log(H.T+ 1e-8))\n",
        "            derivatives = self.backward(X, Y, store)\n",
        " \n",
        "            for l in range(1, self.L + 1):\n",
        "                # gradient descent\n",
        "                ########### complete the code ################\n",
        "\n",
        "                parameters[\"W2\"] = parameters['W2'] - learning_rate * derivatives['dW2']\n",
        "                parameters[\"b2\"] = parameters['b2'] - learning_rate * derivatives['db2']\n",
        "\n",
        "                parameters[\"W1\"] = parameters['W1'] - learning_rate * derivatives['dW1']\n",
        "                parameters[\"b1\"] = parameters['b1'] - learning_rate * derivatives['db1']\n",
        "\n",
        "                ##############################################\n",
        "\n",
        "            if loop % 10 == 0:\n",
        "                print(\"[%3d/%3d] Cost: %.4f\"%(loop, n_iterations, cost),\n",
        "                      \"Train Accuracy: %2.2f\"%(self.predict(X, Y)))\n",
        " \n",
        "            self.costs.append(cost)\n",
        "            H, store = self.forward(X_test)\n",
        "            val_cost = -np.mean(y_test * np.log(H.T+ 1e-8))            \n",
        "            self.val_costs.append(val_cost)\n",
        " \n",
        "    def predict(self, X, Y):\n",
        "        A, cache = self.forward(X)\n",
        "        y_hat = np.argmax(A, axis=0)\n",
        "        Y = np.argmax(Y, axis=1)\n",
        "        accuracy = (y_hat == Y).mean()\n",
        "        return accuracy * 100\n",
        " \n",
        "    def plot_cost(self):\n",
        "        plt.figure()\n",
        "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
        "        plt.plot(np.arange(len(self.val_costs)), self.val_costs)\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"cost\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yTl-FzF9Jqg"
      },
      "source": [
        "Build Model with 50 number of hidden units and train it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "_ta25z2B5Cm0",
        "outputId": "d38edce9-4d92-4594-c220-5502a9f68dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0/200] Cost: 0.1703 Train Accuracy: 27.00\n",
            "[ 10/200] Cost: 0.1956 Train Accuracy: 30.00\n",
            "[ 20/200] Cost: 0.1699 Train Accuracy: 60.00\n",
            "[ 30/200] Cost: 0.1435 Train Accuracy: 68.00\n",
            "[ 40/200] Cost: 0.1210 Train Accuracy: 72.00\n",
            "[ 50/200] Cost: 0.1035 Train Accuracy: 78.00\n",
            "[ 60/200] Cost: 0.0891 Train Accuracy: 82.00\n",
            "[ 70/200] Cost: 0.0765 Train Accuracy: 85.00\n",
            "[ 80/200] Cost: 0.0654 Train Accuracy: 88.00\n",
            "[ 90/200] Cost: 0.0558 Train Accuracy: 93.00\n",
            "[100/200] Cost: 0.0480 Train Accuracy: 96.00\n",
            "[110/200] Cost: 0.0416 Train Accuracy: 98.00\n",
            "[120/200] Cost: 0.0364 Train Accuracy: 98.00\n",
            "[130/200] Cost: 0.0322 Train Accuracy: 99.00\n",
            "[140/200] Cost: 0.0286 Train Accuracy: 100.00\n",
            "[150/200] Cost: 0.0257 Train Accuracy: 100.00\n",
            "[160/200] Cost: 0.0233 Train Accuracy: 100.00\n",
            "[170/200] Cost: 0.0213 Train Accuracy: 100.00\n",
            "[180/200] Cost: 0.0196 Train Accuracy: 100.00\n",
            "[190/200] Cost: 0.0182 Train Accuracy: 100.00\n",
            "Train Accuracy: 100.0000\n",
            "Test Accuracy: 77.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "layers_dims = [50, 10]\n",
        "\n",
        "ann = ANN(layers_dims)\n",
        "ann.fit(X_train, y_train, learning_rate=.01, n_iterations=200)\n",
        "print(\"Train Accuracy: %.4f\" % ann.predict(X_train, y_train))\n",
        "print(\"Test Accuracy: %.4f\" % ann.predict(X_test, y_test))\n",
        "ann.plot_cost()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oi02BR9BdLY9"
      },
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab 3.ipynb의 사본",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}